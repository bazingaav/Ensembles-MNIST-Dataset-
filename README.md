# Ensembles-MNSIT-Dataset-
BE Project (Part 2) - Design and Development of System to Improve Classification Models using Ensemble Learning 

It contains various ensemble learning techniques like voting, bagging, boosting and stacking applied on MNSIT Dataset using algorithms like Logistic Regression, Decision Tree, SVM, KNN, Naive Bayes, Light GBM, XGBoost, Random Forest, Bagging Meta Estimator.  
<br>
The results can be explained as follows:<br>
All the ensemble learning techniques namely voting, bagging, boosting and stacking perform significantly better than the base classifiers. Boosting technique like Light GBM and XGBoost perform better than initial voting model and bagging models and are also significantly fast. Thus boosting models give better result at faster pace. The data was overfitting in decision tree model hence Random Forest performed better than it or else the performances would have been similar as bagging is useful only in case of overfitting and cannot produce significant result like voting and stacking. The maximum accuracy obtained from single-level stacking which consisted of all the best performing classifiers including SVM, KNN, LGBM, Random Forest and XGB is 96.68%. This model contained variations of the base classifiers, thus increasing the number on base classifiers to 13 on level 0. It is to be noted that, when we used relatively strong classifier at lower level, accuracy of stacking increases. On the other hand, in 2-level stacking, level 0 had 22 classifiers, level 1 had 6 classifiers and Random Forest and LGBM on level 2 which gave the accuracy of 97.5 %. Thus, increasing the amount of models on level 0 increased the accuracy by about 0.82%. These base classifiers on level 0 are variants of the general base classifiers mentioned above, derived by using hyperparameter tuning and feature selection. This model is performing better as the variants cover a wide range variety of data and aren’t repetitive or else the performance wouldn’t have been affected much. Also, when single-level stacking and voting of same classifiers namely SVM, KNN, LGBM and Random Forest are compared stacking is found to perform better than voting approach if base models are strong (usually boosting models at lower levels). Thus, the addition of a strong classifier led to increases in the accuracy of the stacked model. Thus, from results, it can be concluded that heterogenous stacking approach with boosting models in lower levels help to increase performance. It is important to note that, one of the main reasons of increased accuracy is variety of models we have used. If a single model is strong enough and covers all variations in data, then in such cases ensembles might not increase accuracy. 
